<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>106A Project</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

     <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>

     <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>

     <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
     <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>

     <script src="js/modernizr-2.6.2.min.js"></script>

     <link rel="stylesheet" href="{{ url_for('static',     filename='css/template.css') }}">
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
  </head>
  <body>
    {% extends "template.html" %}
    {% block content %}

    <br>
    <br>
    <br>

    <div class="container" >

      <h1 class="display-4"> EECS 106A Group Project </h1>
      <br>
      <blockquote class="blockquote text-center">
        <p class="mb-0"> "A cool quote about drones and robotics"
        </p>
      </blockquote>
      <br>

      <h1 class="display-4"> Project Aim </h1>
      <p class="lead">
        <br>

        <u>
        Describe the end goal of your project.
        </u>

        <br>
        The goal of our project is to build an end-to-end prototype for semi-autonomous quadrotor delivery. We envision having a simple app allowing users to place an order with their coordinates for delivery. This request will then appear in the operator’s VR environment enabling them to create a waypoint path for the quadrotor to approach the delivery destination. The final waypoint is placed such that it leads the quadrotor close enough to the delivery destination, yet far enough from the densely populated area. It is up to the robot, then, to autonomously and safely navigate the area, using its sensors and intelligence. Its goal is to locate a QR code that has been placed by the user to mark the exact delivery location and deliver the package. Furthermore, an internal goal for our team is to deploy this application on the ISAACS server.
        <br>
        <br>

        <u>
        Why is this an interesting project? What interesting problems do you need to solve to make your solution work?
        </u>

        <br>
        The FDA recently approved Amazon and UPS to experiment with UAV package delivery. We envision that by 2030, the vast majority of commodity trade will be accomplished by drones operating semi-autonomously. To achieve this, there are two key problems that need to be addressed:
        <br>
        1. The cognitive overload an operator faces in managing drones when they fly beyond line-of-sight.
        <br>
        2. The ability of a quadrotor to intelligently maneuver within a densely populated area as it approaches its drop-off target.
        <br>
        We therefore want to utilize our team’s unique background and position to create a proof of concept application for semi-autonomous package delivery using Virtual Reality and the DJI Matrice 210 quadrotors to solve these interesting problems.
        <br>
        <br>

        <u>
        In what real-world robotics applications could the work from your project be useful?
        </u>

        <br>
        The direct real world application we are focusing on is semi-autonomous drone package delivery. We further believe that different components of our work could be used in other industries as well. The autonomous search algorithm and control code can be adapted to work for search and rescue missions in hard to reach terrain and environments. The 3D real-time visualization in virtual reality can also help augment search and resume missions by providing a more intuitive environment for operators to conduct these missions. Finally, we can also see potential uses of our image segmentation algorithm in the agricultural field for precision agriculture use cases.
        <br>
        <br>

      </p>
      <br>

      <h1 class="display-4"> Design </h1>
      <p class="lead">
        <u>
        (a) What design criteria must your project meet? What is the desired functionality?
        </u>

        <br>
        The following key features (in order) comprise the design criteria our project must meet to be a successful MVP for a semi-autonomous delivery system:
        <br>
        A customer places an order with their world coordinates.
        <br>
        An operator in VR creates a waypoint mission for the quadrotor to the user area.
        <br>
        The operator in VR creates a search area and uploads the mission to the quadrotor
        <br>
        The quadrotor flies to the target area, following the waypoint mission.
        <br>
        The quadrotor autonomously searches the target area by creating an occupancy grid of obstacles using onboard cameras and performing a depth first search.
        <br>
        The quadrotor detects a QR code indicating the precise location of the delivery and lands.
        <br>
        <br>

        <u>
        (b) Describe the design you chose.
        </u>

        <br>
        Due to the various technologies and components involved in our system, we choose a modular system architecture to streamline unit testing, ease of integration and future scalability.
        <br>
        <br>

        Web App:
        <br>
        The Web application is built using Flask and deployed on Heroku. This was done to create a functional live application with minimal effort to focus on the more challenging aspects of the project. It supports a simple form to input a user’s order and sends this to the virtual reality application.
        <br>
        <br>

        VR:
        <br>
        The virtual reality application is built on Unity and is an extension of the ISAACS research project. This approach was chosen as the ISAACS project provides the basic functionality required for the VR application to interface with the Matrice 210 drone. For this project, we had to create the HTTP endpoints for the web application to send the user information to the virtual reality application. We then had to extend the ISAACS architecture to support the autonomous search functionality that can be controlled by the user. This encapsulated creating the UI features, ROS service calls to the drone for waypoint missions and our custom node for search missions, and unit testing for all new features.
        <br>
        <br>

        Drone hardware:
        <br>
        We are using the DJI Matrice 210 as our physical drone. It has the Manifold 2-C as its onboard computer running the DJI Onboard SDK, as well our ROS node isaacs_autonomy. The drone has an RTK GPS, which communicates with an on-ground RTK base station. It also has stereoscopic cameras that we are using to create a depth map.
        <br>
        <br>

        Image processing:
        <br>
        Apollo Thomopoulos
        <br>
        <br>

        2D occupancy grid:
        <br>
        Apollo Thomopoulos
        <br>
        <br>

        Exploration algorithm + control:
        <br>
        The explore algorithm is an extension of the “flood fill” algorithm. It uses a 2D occupancy grid to determine its next move. It is a recursive algorithm that performs a depth first search.
        <br>
        For drone controls, the operator sets waypoints. These waypoints are sent to the DJI SDK, and the drone then flies to each of the waypoints. These waypoints can be spaced far apart. For the search algorithm, however, we can’t just use waypoints as we don’t know how much space is open, or whether there is an obstacle on the way to our destination. This information only comes in as we fly. So instead, we control the drone with much finer, incremental movements in the XY plane. For our purposes, we are flying at a fixed height, so we don’t need to move in the Z direction. By moving incrementally in the XY plane, we never commit to moving a large distance. Instead, we move a small amount (which we know is unoccupied), update our occupancy grid, and then make the next decision.
        <br>
        <br>

        <u>
        (c) What design choices did you make when you formulated your design? What trade-offs did you have to make?
        </u>

        <br>
        Web App:
        <br>
        The three key components of the web application were a user interface to input an order, ability to relay this information to the virtual reality application and to deploy this application. These were accomplished efficiently by Flask and Heroku and hence we choose these over other alternatives such as Django, Node.js, AWS, GCP, etc.
        <br>
        <br>

        VR Application:
        <br>
        The three key components of the virtual reality application were the HTTP endpoint for the web application, the user interface for the operator to define and control the search mission, and finally the ROS architecture to relay these commands to the DJI SDK. The key design choice we had was to either extend the abstract drone & user interface classes to ensure future scalability at the cost of time or develop these features as one-offs faster. As we wish to continue this project past the class and contribute to the research project we decided to extend the current drone abstractions to support a search algorithm which can be customized for each drone as needed, with our search algorithm chosen for the Matrice 210 drone. We did the same for the user interface so that the ability to create a search region and relay it to a drone is scaled to all connected drones instead of just the Matrice 210. Finally, we added the custom ROS service calls, message types and service definitions as abstract classes/methods that can be customized by to scale for other drones and projects.
        <br>
        <br>

        Drone:
        <br>
        In our lab, we had access to the DJI Matrice 210, Matrice 600, and Phantom drones.
        The M210 and M600 have an easy to use Onboard SDK that lets us use ROS to control the drones, and publishes various drone state information (battery life, GPS position, camera stream). This eliminated the Phantom as a choice. The M210 and M600 also both have RTK GPS, which lets us easily localize the drone.
        The M210 has on-board stereo cameras, which was the main reason we chose to use it over the M600. Additionally, the M600 is much bigger, and more expensive, so we didn’t want to risk using it. Since the M210 is smaller, it can reach more places and fit in tighter spots than the M600.
        <br>
        <br>

        Image Processing
        <br>
        Apollo Thomopoulos
        <br>
        <br>

        2D Occupancy Grid Node
        <br>
        Apollo Thomopoulos
        <br>
        3D
        More space
        Need more FOV on cameras (how do we know the area directly above the drone is empty)
        More complicated search algorithm
        And at the end of the day, it's not much more effective
        Only benefit is maneuvering around complex shaped obstacles
        Not viable in the time period
        Have access to drone’s position and attitude, so easy to fill in the grid
        <br>
        The main reason for choosing the flood fill algorithm was that it searches every single open space, while avoiding all obstacles. It also lets us constantly update the occupancy grid as we fly. The drone control it uses (moving forward/backward/left/right one cell at a time) also integrated well with the DJI SDK’s control API. In addition, and very importantly, the flood fill algorithm is very robust, reliable, and easily visualized. We could have tried using a machine learning model, but those aren’t as robust, and we can’t know for sure why the drone chose its next move. With the flood fill, we know how it chooses the next move, and can predict what it’ll do next. With an expensive physical drone, this is highly valuable as we can’t risk any unpredictable behavior that can cause a crash.
        <br>
        <br>

        <u>
        (d) How do these design choices impact how well the project meets design criteria that would be encountered in a real engineering application, such as robustness, durability, and efficiency?
        </u>
        <br>
        Overall we did well.
        <br>
        <br>

      </p>
      <br>

      <h1 class="display-4"> Implementation </h1>
      <p class="lead">
        <br>

        <u>
        (a) Describe any hardware you used or built. Illustrate with pictures and diagrams.
        </u>

        <br>
        Drone:
        <br>
        Drone: DJI Matrice 210
        <br>
        Onboard sensors: RTK GPS, stereo cameras
        <br>
        Onboard computer: Manifold 2-C
        <br>
        Software: DJI Onboard SDK
        <br>
        On the ground: RTK base station (communicates with on-board RTK GPS for centimeter-accuracy location data)
        <br>
        <br>

        VR:
        <br>
        We used the Facebook Oculus Rift as the Virtual Reality Headset which comes equipped with two handheld controllers. The virtual reality application requires a VR ready laptop with the ability to support the Oculus headset, and we used the Alienware M15 R3 laptop to deploy the virtual reality application.
        <br>
        <br>

        <u>
        (b) What parts did you use to build your solution?
        </u>

        <br>
        Commercial off the shelf parts listed above
        <br>
        <br>

        <u>
        (c) Describe any software you wrote in detail. Illustrate with diagrams, flow charts, and/or other appropriate visuals. This includes launch files, URDFs, etc.
        </u>

        <br>
        Web App
        <br>
        The web application was built using Flask, HTML, CSS and Bootstrap. It is a 2 page website with the first page comprising the project report created using HTML/CSS. The second page has an input form using Bootstrap components and uses an HTTP POST request to send this form to the virtual reality application. This is deployed on Heroku which abstracts away all hosting code.
        <br>
        <br>

        VR:
        <br>
        The virtual reality application was built using Unity, C#, the ROSBridgeWebsocket library and numerous Unity assets. We used the .NET unity library to create a REST API endpoint for the server to send the user information to with C# code to parse the response and visualize a pointer for the VR operator to be informed of the user’s general location. Unity assets for 3D buttons along with C# scripts are used to create the user interface to enable operators to create and define a search region in the virtual reality interface. The drone class to create the virtual drone gameobject is written to spawn a websocket connection to the drone’s onboard computer using the ROSBridgeWebsocket library. This class subscribes to the drone’s position, localizes it and constantly updates the VR user interface to inform the operator of a drone’s real-world position. This class also contains all the functionality to call ROS services hosted on the drone’s onboard computer and is used to trigger functionality as per the VR operators needs. We further created the abstract custom service calls and message types classes that can scale as required.
        <br>
        <br>


        ROS “isaacs_autonomy” Node essentially does 3 things: All written in Python
        <br>
        Image Processing
        <br>
        Apollo Thomopoulos
        <br>
        Occupancy grid
        <br>
        Apollo Thomopoulos
        <br>
        Search algo
        <br>
        The search algorithm takes in a radius in meters as a parameter. It then attempts to traverse every cell in that radius, unless there is an obstacle. It uses the occupancy grid to understand the environment (is a space occupied/unoccupied/unknown), and has its own 2d array of visited/unvisited cells.
        <br>
        It is a recursive algorithm that checks the appropriate neighbor’s cell and if it's unoccupied+unvisited:
        <br>
        Moves up, and recurses.
        <br>
        Moves down, and recurses
        <br>
        Moves right, and recurses
        <br>
        Moves left, and recurses
        <br>
        It is a depth first search as it visits a cell’s neighbors’ neighbors before visiting all of its own neighbors. It also updates the occupancy grid as the drone moves, to ensure we have up-to-date info about obstacles.
        <br>
        <br>

        <u>
        (d) How does your complete system work? Describe each step.
        </u>
        <br>
        1. A customer places an order with their world coordinates.
        <br>
        2. An operator in VR creates a waypoint mission for the quadrotor to the user area.
        <br>
        3. The operator in VR creates a search area and uploads the mission to the quadrotor
        <br>
        4. The quadrotor flies to the target area, following the waypoint mission.
        <br>
        5. The quadrotor autonomously searches the target area by creating an occupancy grid of obstacles using onboard cameras and performing a depth first search.
        <br>
        6. [Ran out of time] The quadrotor detects a QR code indicating the precise location of the delivery and lands.
        <br>

      </p>
      <br>

      <h1 class="display-4"> Project Results </h1>
      <p class="lead">
        <br>

        <u>
        (a) How well did your project work? What tasks did it perform?
        </u>

        <br>
        Overall, we are very happy with the progress we made over the course of the semester. All our key components (VR, Vision, Search, Hardware) are successfully integrated and worked. We wish to conduct a real-world flight test when permitted by COVID-19 regulations but based on the success in the realistic DJI sim we are confident that our application will succeed.
        <br>
        <br>

        <u>
        (b) Illustrate with pictures and at least one video
        </u>
        <br>
        </p>
        <div class="embed-responsive embed-responsive-16by9">
          <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTX-ST1ma7Gus38mT3FtR6tQWuuuqaTFFEDIdOWAPMphmtRBFyBLJKavII0yZ4iiUTYGmzwPh8y1LBT/embed?start=false&loop=false&delayms=10000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
        </div>
        <br>

        <p class="lead">
          <br>
          A video demo of the full project: (TODO: Update)
        </p>

        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/gZLGlP98EsA" allowfullscreen></iframe>
        </div>

        <br>
        <br>
        <br>

      <h1 class="display-4"> Conclusion </h1>
      <p class="lead">
        <br>

        <u>
        (a) Discuss your results. How well did your finished solution meet your design criteria?
        </u>

        <br>
        Overall, we are very happy with the progress we made over the course of the semester. All our key components (VR, Vision, Search, Hardware) are successfully integrated and worked. We wish to conduct a real-world flight test when permitted by COVID-19 regulations but based on the success in the realistic DJI sim we are confident that our application will succeed.
        <br>
        We understand that flying in sim can sound disappointing, and obviously it's different from flying in the real world. But at the same time, we want to emphasize a few things:
        <br>
        The DJI Assistant 2 For Matrice simulator is not just any simulator, it is an industry grade simulator created by DJI themselves specifically for their drones. The Simulator only works when plugged into the physical drone as all physics, flight controller, measurement and other calculations take place on the drone’s flight controller itself. The flight controller is publishing and supporting all the same ROS functionality for control as a real flight, with only the rotors and gps data being simulated. In previous projects, after achieving successful flights in the DJI simulator, they have always transferred into successful real-world tests and therefore we are confident that we can achieve a full demo flight when we are able to conduct one.
        <br>
        <br>

        <u>
        (b) Did you encounter any particular difficulties?
        </u>

        <br>
        Calibrating real world camera is annoying
        <br>
        Drone cameras were not the best
        <br>
        Creating local networks for drone flights is painful
        <br>
        Setting up and testing with the DJI sim is also painful
        <br>
        Testing the DJI SDK was painful
        <br>
        Limited access to equipment/only 2 days in RFS
        <br>
        When we did have access, had to keep recharging batteries (drone batteries aren't great)
        <br>
        Limited daylight to fly drones
        <br>
        GPS hardware localization was spotty
        <br>
        Flight checklist/safety precautions
        <br>
        Drone is expensive
        <br>
        can’t just go out and fly autonomously
        <br>
        Cant make small changes and keep testing
        <br>
        With ML models, you crash a million times before it works once
        <br>
        With physical, expensive drones, has to work every single time
        <br>
        Need robust calibration
        <br>
        x/y flight test
        <br>
        Set waypoints directly above 4 corners of the building, and make sure it lines up
        <br>
        Need spotters from different angles because depth perception is weird
        <br>
        Altitude test
        <br>
        Set waypoints at window height to make sure altitude matches
        <br>
        Now that we trust x/y/z conversion between real world GPS and our Unity VR interface, we can actually run flight tests
        <br>
        <br>

        <u>
        (c) Does your solution have any flaws or hacks? What improvements would you make if you had additional time?
        </u>
        <br>
        Every time we fly needs re-calibration of GPD with our VR interface. Even though our interface has fixed coordinates, and the GPS gives us absolute real-world coordinates, for some reason they wouldn't line up. As an example, if we place the drone by the corner of a building, and convert to our unity coordinates, it correctly shows up in our unity visualization. But if we turn everything off, and then on again, the drone shows up 15 feet off in our visualization, so we have to recalibrate everything.
        <br>
        <br>

      </p>
      <br>


      <h1 class="display-4"> The Team </h1>
      <p class="lead">
        <br>
        The following UC Berkley students developed this project for the EECS 106A Class.
      </p>

      <br>

      <div class="card-deck">
        <div class="card">
          <img class="card-img-top" src="{{ url_for('static', filename='peru.jpg') }}" alt="Card image cap">
          <div class="card-body">
            <h5 class="card-title">Peru Dayani</h5>
            <p class="card-text">
            -- Placeholder for Role --
            <br>
            -- Placeholder for Work Done --
            </p>
            <p class="card-text">
            EECS @ UC Berkeley
            </p>
            <a href="https://www.linkedin.com/in/peru-dayani-09367314a/" class="btn btn-primary">LinkedIn</a>
          </div>
        </div>
        <div class="card">
          <div style="width:349px ;height:350px ;overflow:hidden">
            <img class="card-img-top" src="{{ url_for('static', filename='peru.jpg') }}" alt="Card image cap">
          </div>
          <div class="card-body">
            <h5 class="card-title">Varun Saran</h5>
            <p class="card-text">
            -- Placeholder for Role --
            <br>
            -- Placeholder for Work Done --
            </p>
            <br>
            <p class="card-text">
            EECS @ UC Berkeley
            </p>
            <a href="https://www.linkedin.com/in/varunsaran/" class="btn btn-primary">LinkedIn</a>
          </div>
        </div>
        <div class="card">
          <div style="width:349px ;height:350px ;overflow:hidden">
            <img class="card-img-top" src="images/Apollo.png" alt="Card image cap">
          </div>
          <div class="card-body">
            <h5 class="card-title">Apollo θ</h5>
            <p class="card-text">
            Vision and Integration
            <br>
            - Hardware Setup
            - System Integration
            - Sensor Calibration
            - Sensor Streams
            - Image Processing
            - Occupancy Grid
            </p>
            <p class="card-text">
            EECS @ UC Berkeley
            </p>
            <a href="https://apollovision.github.io/" class="btn btn-primary">Website</a>
          </div>
        </div>
      </div>

      <br>
      <br>
      <br>

      <h1 class="display-4"> Additional Materials </h1>
      <p class="lead">
        <br>
        Github Links:
        Autonomy Package: https://github.com/apollovision/isaacs_autonomy
        VR Interface: https://github.com/immersive-command-system/ImmersiveDroneInterface_2/tree/106a-proj
        <br>
        Hardware Information:
        M210:
        Product: https://www.dji.com/matrice-200-series
        Specs: https://www.dji.com/matrice-200-series/info#specs
        RTK Station:
        Product: https://www.dji.com/d-rtk-2
        Manifold 2C:
        Product: https://www.dji.com/manifold-2
        Specs: https://www.dji.com/manifold-2/specs
        DJI Simulator:
        Product: https://www.dji.com/downloads/softwares/assistant-dji-2-for-matrice
      </p>
      <br>

    </div>

    {% endblock %}
  </body>
</html>
