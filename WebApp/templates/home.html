<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>106A Project</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

     <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>

     <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>

     <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
     <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>

     <script src="js/modernizr-2.6.2.min.js"></script>

     <link rel="stylesheet" href="{{ url_for('static',     filename='css/template.css') }}">
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
  </head>
  <body>
    {% extends "template.html" %}
    {% block content %}

    <br>
    <br>
    <br>

    <div class="container" >

      <h1 class="display-4"> Autonomous UAV Package Delivery </h1>
      <br>
      <blockquote class="blockquote text-center">
        <p class="mb-0"> "by the folks of the Immersive Semi-Autonomous Aerial Command System"
        </p>
      </blockquote>
      <br>


      <!--
        <br>
        <h3 align="middle">0. Setup sensor streams to receive readings for 3D position, 3D Euler angles, and a front disparity image. </h3>
        <h3 align="middle">1. Process the disparity image with a median filter to remove noise. </h3>
        <h3 align="middle">2. Warp the disparity image using a 3D-rototranslational model, to account for the quadrotor's elevation, pitch and roll. </h3>
        <br>

        <div align="middle">
          <table style="width=100%">
            <tr>
            <td>
              <img src="static/adjust1.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            <td>
              <img src="static/adjust2.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            </tr>
            <tr>
            <td>
              <img src="static/adjust3.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            <td>
              <img src="static/adjust4.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            </tr>
          </table>
        </div>

        <br>
        <br>
        <h3 align="middle">3. The disparity map rows are now parallel to the grid plane. Select only a small number of them, close to the intersection.</h3>

        <h3 align="middle">4. Max pool vertically from the selected rows, in order to "collapse" them into a 1D array of depths.</h3>
        <br>

        <div align="middle">
          <table style="width=100%">
            <tr>
            <td>
              <img src="static/disparity.gif" class="thumbnail" align="middle" width="700px"/>
            </td>
            </tr>
          </table>
        </div>

        <br>
        <br>
        <h3 align="middle">5. Project the quadrotor's field of view on the grid plane, and use the 1D array of depths to localize any obstacles within it. </h3>
        <h3 align="middle">6. Raytrace the quadrotor's field of view projection, to update the occupancy grid.</h3>
        <br>



        <div align="middle">
          <table style="width=100%">
            <tr>
            <td>
              <img src="static/map1.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            <td>
              <img src="static/map3.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            </tr>
          </table>
        </div>
        <br>
        <br>
        <div align="middle">
          <table style="width=100%">
            <tr>
            <td>
              <img src="static/grid.gif" class="thumbnail" align="middle" width="700px"/>
            </td>
            </tr>
          </table>
        </div>
        <br>
        <br>
        <div align="middle">
          <table style="width=100%">
            <tr>
            <td>
              <img src="static/map2.png" class="thumbnail" align="middle" width="600px"/>
            </td>
            </tr>
          </table>
        </div>
    -->


      <h1 class="display-4"> Project Aim </h1>
      <p class="lead">
        <br>

        <u>
        Describe the end goal of your project.
        </u>

        <br>
        The goal of our project is to build an end-to-end prototype for semi-autonomous quadrotor delivery. We envision having a simple app allowing users to place an order with their coordinates for delivery. This request will then appear in the operator’s VR environment enabling them to create a waypoint path for the quadrotor to approach the delivery destination. The final waypoint is placed such that it leads the quadrotor close enough to the delivery destination, yet far enough from the densely populated area. It is up to the robot, then, to autonomously and safely navigate the area, using its sensors and intelligence. Its goal is to locate a QR code that has been placed by the user to mark the exact delivery location and deliver the package. Furthermore, an internal goal for our team is to deploy this application on the ISAACS server.
        <br>
        <br>

        <u>
        Why is this an interesting project? What interesting problems do you need to solve to make your solution work?
        </u>

        <br>
        The FDA recently approved Amazon and UPS to experiment with UAV package delivery. We envision that by 2030, the vast majority of commodity trade will be accomplished by UAVs operating semi-autonomously. To achieve this, there are two key problems that need to be addressed:
        <br>
        1. The cognitive overload an operator faces in managing UAVs when they fly beyond line-of-sight.
        <br>
        2. The ability of a quadrotor to intelligently maneuver within a densely populated area as it approaches its drop-off target.
        <br>
        We therefore want to utilize our team’s unique background and position to create a proof of concept application for semi-autonomous package delivery using Virtual Reality and the DJI Matrice 210 quadrotors to solve these interesting problems.
        <br>
        <br>

        <u>
        In what real-world robotics applications could the work from your project be useful?
        </u>

        <br>
        The direct real world application we are focusing on is semi-autonomous UAV package delivery. We further believe that different components of our work could be used in other industries as well. The autonomous search algorithm and control code can be adapted to work for search and rescue missions in hard to reach terrain and environments. The 3D real-time visualization in virtual reality can also help augment search and resume missions by providing a more intuitive environment for operators to conduct these missions. Finally, we can also see potential uses of our image segmentation algorithm in the agricultural field for precision agriculture use cases.
        <br>
        <br>

      </p>
      <br>

      <h1 class="display-4"> Design </h1>
      <p class="lead">
        <u>
        (a) What design criteria must your project meet? What is the desired functionality?
        </u>

        <br>
        The following key features (in order) comprise the design criteria our project must meet to be a successful MVP for a semi-autonomous delivery system:
        <br>
        A customer places an order with their world coordinates.
        <br>
        An operator in VR creates a waypoint mission for the quadrotor to the user area.
        <br>
        The operator in VR creates a search area and uploads the mission to the quadrotor
        <br>
        The quadrotor flies to the target area, following the waypoint mission.
        <br>
        The quadrotor autonomously searches the target area by creating an occupancy grid of obstacles using onboard cameras and performing a depth first search.
        <br>
        The quadrotor detects a QR code indicating the precise location of the delivery and lands.
        <br>
        <br>

        <u>
        (b) Describe the design you chose.
        </u>

        <br>
        <br>

        <img src="static/system.png" align="middle" width="1100px">

        <br>
        Due to the various technologies and components involved in our system, we choose a modular system architecture to streamline unit testing, ease of integration and future scalability.
        <br>
        <br>

        <b>Web App:</b>
        <br>
        The Web application is built using Flask and deployed on Heroku. This was done to create a functional live application with minimal effort to focus on the more challenging aspects of the project. It supports a simple form to input a user’s order and sends this to the virtual reality application.
        <br>
        <br>

        <b>VR Interface:</b>
        <br>
        The virtual reality application is built on Unity and is an extension of the ISAACS research project. This approach was chosen as the ISAACS project provides the basic functionality required for the VR application to interface with the Matrice 210 UAV. For this project, we had to create the HTTP endpoints for the web application to send the user information to the virtual reality application. We then had to extend the ISAACS architecture to support the autonomous search functionality that can be controlled by the user. This encapsulated creating the UI features, ROS service calls to the UAV for waypoint missions and our custom node for search missions, and unit testing for all new features.
        <br>
        <br>

        <b>Drone hardware:</b>
        <br>
        We are using the DJI Matrice 210 as our physical UAV. It is a small-sized UAV, which for the purpose of our project, can be modelled as a sphere of 0.887 m radius. Moreover, it is mounted with the Manifold 2-C onboard computer running the DJI Onboard SDK, as well as our ROS node isaacs_autonomy. The UAV has an RTK GPS, which communicates with an on-ground RTK base station and publishes at a rate of 50 Hz. It also has a 100 Hz gyroscope, and a pair of 240p 60×54° FOV stereoscopic cameras publishing at 10 Hz. We are furthermore able to extract from the cameras a disparity image that provides an accurate scan of the environment at 0.7 and upto 30 m away from the quadrotor.
        <br>
        <br>

        <b>Calibration:</b>
        <br>
        We calibrate the sensor readings such that they provide information relative to the reference frame from which the quadrotor will begin its search. We also make certain unit conversions to facilitate our work. In more detail, we convert the gyroscope’s readings from quaternions to Eulers angles, and the RTK GPS’ readings from LLA of a WGS84 ellipsoid to Cartesian ENU.
        <br>
        <br>

        <b>Vision Problem Statement:</b>
        <br>
        The goal of the vision system we built is to quickly and accurately update a 2D occupancy grid that the quadrotor can use  to navigate. The grid is initiated as a 2D array of signed 8-bit integers, with each entry representing a discrete unit equal to the qudrotor’s diameter (0.887 m). Positive values in an entry indicate that this cell is free, while negative values that it is occupied, and therefore inaccessible. To update the grid, a 360×240 disparity image is available at a rate of 10 Hz, along with potentially helpful information from the RTK GPS and Gyroscope.
        <br>
        <br>

        <b>Image Processing:</b>
        <br>
        <h3 align="middle">1. Process the disparity image with a median filter to remove noise. </h3>
        We begin by blurring the disparity image with a median filter of kernel size 5, in order to eliminate noise. The choice of filter was made taking into consideration both the quality of the resulting image, and actual runtime. In practice, its major competitor, the Gaussian filter, was faster, but not faster enough to justify its use over the median filter – it provided less than 50% speedup at the cost of a significantly more noisy image or information loss (when its standard deviation became very large, the disparity map turned uniformly gray).
        <br>
        <h3 align="middle">2. Warp the disparity image using a 3D-rototranslational model, to account for the quadrotor's elevation, pitch and roll. </h3>
        Next, once the denoising was over, we had to account for the fact that as the UAV rotated around its three principal axes—or moved along them—, the disparity image we received would correspond to different places in the world. Therefore, its geometric relationship with the plane of interest (the one where we build the occupancy grid), differs from the one that the disparity image taken in the reference frame has with that very plane. Put simply, in the reference frame, the middle row of the disparity image is where the image intersects the grid plane, and the few rows above and below provide an envelope for the quadrotor’s height (note: as such the occupancy grid plane is not really a plane, but a cuboid with a very small height – more on this later). In the body frame, however, the occupancy grid plane may correspond to a diagonal in the disparity image.
        <br>
        As such, before using the disparity map, we had to warp it using a 3D roto translational model, similar to one we would use to change coordinate frames. In this case, however, the story was a bit more complicated than multiplying by an SO3 matrix, as we first had to recover the camera’s intrinsic parameters, to accurately represent the 2D to 3D and then 3D to 2D reprojection.
        Moreover, we had to ensure that the pixel mapping between projections preserved as much information as possible. We therefore used an inverse warp alongside a Lancosz interpolation. The latter was chosen over the faster bilinear interpolation because of the higher quality results it tends to produce. Similarly to the median filter, the performance hit was small enough in comparison to the significance of the information preserved to justify its use. All in all, after the full round of processing, the disparity image was still available at a rate of 10 Hz.
        <br>
        <br>
        <div align="middle">
          <table style="width=100%">
            <tr>
            <td>
              <img src="static/adjust1.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            <td>
              <img src="static/adjust2.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            </tr>
            <tr>
            <td>
              <img src="static/adjust3.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            <td>
              <img src="static/adjust4.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            </tr>
          </table>
        </div>

        <br>
        <br>

        <b>Occupancy Mapping:</b>
        <br>
        <h3 align="middle">3. The disparity map rows are now parallel to the grid plane. Select only a small number of them, close to the intersection.</h3>

        <h3 align="middle">4. Max pool vertically from the selected rows, in order to "collapse" them into a 1D array of depths.</h3>
        <br>
        Upon receiving the processed disparity map, we select a small region around its middle row, and discard the rest. This is because, after the processing, we know that the middle row corresponds to where the disparity image intersects the grid plane, and the few rows above and below can be useful as the UAV has a non-negligible height, and they can also be used to correct potential errors. In practice, we discard about 60% of the original disparity map, and then do a max pool along each column of the remaining 40%. The max pool is chosen over the weighted average, or any other downsampling operation, because even if a very small depth (= large disparity value) is present on the very edge of the selected region, the quadrotor may crash in it (again, the selected region “envelopes” the quadrotor plus accounts for a margin of error).

        <br>
        <div align="middle">
          <table style="width=100%">
            <tr>
            <td>
              <img src="static/disparity.gif" class="thumbnail" align="middle" width="700px"/>
            </td>
            </tr>
          </table>
        </div>

        <br>

        <h3 align="middle">5. Project the quadrotor's field of view on the grid plane, and use the 1D array of depths to localize any obstacles within it. </h3>
        <h3 align="middle">6. Raytrace the quadrotor's field of view projection, to update the occupancy grid.</h3>

        <br>
        Once the max pool has finished, what we are essentially left with is a 1D array of depths, indicating the obstacles to be found within the quadrotor’s FOV projected upon the grid plane. As such, the grid update problem now reduces to 2D raytracing, where each depth reading gets corresponded to an angle, such that the ray from the quadrotor’s camera along that angle draws the map in that direction. For the raytrace, we use OpenCV’s draw functions. This is perhaps the heavier part of the code, but is still fairly fast, having a period of less than 0.1s. In the end, our 2D occupancy grid is updated at about 9 Hz, which, considering our actual camera rate of 10 Hz, is excellent.
        <br>
        <div align="middle">
          <table style="width=100%">
            <tr>
            <td>
              <img src="static/map1.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            <td>
              <img src="static/map3.png" class="thumbnail" align="middle" width="400px"/>
            </td>
            </tr>
          </table>
        </div>
        <br>
        <br>
        <div align="middle">
          <table style="width=100%">
            <tr>
            <td>
              <img src="static/grid.gif" class="thumbnail" align="middle" width="700px"/>
            </td>
            </tr>
          </table>
        </div>
        <br>
        <br>
        <div align="middle">
          <table style="width=100%">
            <tr>
            <td>
              <img src="static/map2.png" class="thumbnail" align="middle" width="600px"/>
            </td>
            </tr>
          </table>
        </div>

        <br>
        <br>

        <b>Exploration algorithm + control</b>:
        <br>
        The explore algorithm is an extension of the “flood fill” algorithm. It uses a 2D occupancy grid to determine its next move. It is a recursive algorithm that performs a depth first search.
        <br>
        For UAV controls, the operator sets waypoints. These waypoints are sent to the DJI SDK, and the UAV then flies to each of the waypoints. These waypoints can be spaced far apart. For the search algorithm, however, we can’t just use waypoints as we don’t know how much space is open, or whether there is an obstacle on the way to our destination. This information only comes in as we fly. So instead, we control the UAV with much finer, incremental movements in the XY plane. For our purposes, we are flying at a fixed height, so we don’t need to move in the Z direction. By moving incrementally in the XY plane, we never commit to moving a large distance. Instead, we move a small amount (which we know is unoccupied), update our occupancy grid, and then make the next decision.
        <br>
        <br>

        <u>
        (c) What design choices did you make when you formulated your design? What trade-offs did you have to make?
        </u>

        <br>
        <b>Web App:</b>
        <br>
        The three key components of the web application were a user interface to input an order, ability to relay this information to the virtual reality application and to deploy this application. These were accomplished efficiently by Flask and Heroku and hence we choose these over other alternatives such as Django, Node.js, AWS, GCP, etc.
        <br>
        <br>

        <b>VR Application:</b>
        <br>
        The three key components of the virtual reality application were the HTTP endpoint for the web application, the user interface for the operator to define and control the search mission, and finally the ROS architecture to relay these commands to the DJI SDK. The key design choice we had was to either extend the abstract UAV & user interface classes to ensure future scalability at the cost of time or develop these features as one-offs faster. As we wish to continue this project past the class and contribute to the research project we decided to extend the current UAV abstractions to support a search algorithm which can be customized for each UAV as needed, with our search algorithm chosen for the Matrice 210 UAV. We did the same for the user interface so that the ability to create a search region and relay it to a UAV is scaled to all connected UAVs instead of just the Matrice 210. Finally, we added the custom ROS service calls, message types and service definitions as abstract classes/methods that can be customized by to scale for other UAVs and projects.
        <br>
        <br>

        <b>UAV:</b>
        <br>
        In our lab, we had access to the DJI Matrice 210, Matrice 600, and Phantom UAVs.
        The M210 and M600 have an easy to use Onboard SDK that lets us use ROS to control the UAVs, and publishes various UAV state information (battery life, GPS position, camera stream). This eliminated the Phantom as a choice. The M210 and M600 also both have RTK GPS, which lets us easily localize the UAV.
        The M210 has on-board stereo cameras, which was the main reason we chose to use it over the M600. Additionally, the M600 is much bigger, and more expensive, so we didn’t want to risk using it. Since the M210 is smaller, it can reach more places and fit in tighter spots than the M600.
        <br>
        <br>

        Vision:
        <br>
        Other than the aforementioned implementation trade-offs, the very fact of utilising a 2D grid came with its own set of benefits and flaws. On the one hand, fixing the flight height meant that the totality of vision would mostly reduce to projective geometry, which guarantees robustness, because its mathematics are deterministic. On the other hand, given that the quadrotor’s dynamics are inherently 3-dimensional, the perception system becomes a bottleneck to the actuation system. This is especially true for a quadrotor, which, being an underactuated nonlinear system, relies on linear combinations of rotations along its principal axes to perform translational movement. Put simply, the quadrotor can move along the plane normal to its thrust only by changing its attitude, which translates very poorly to 2D.
        <br>
        Ultimately, we chose to go with 2D because it would make it easier to write a search algorithm for it, and our quadrotor’s SDK abstracts away most of the dynamics. Furthermore, given that our goal was to create a proof-of-concept system, we thought that integration matters more than the individual parts: having a fully working system with 2D navigation would be a greater success than individually, but not collectively, working 3D vision and navigation. Moreover, going with 3D vision would almost certainly necessitate the presence of segmentation, which is extremely compute-heavy. The onboard computer we use only has a CPU, and therefore can hardly support any operations that rely on high parallelisation.
        <br>
        We furthermore have to note that the cameras we had at our disposal were barely able to support 2D vision, not to mention 3D vision. They were, for one, extremely FOV limited, very low refresh-rate, and very prone to noise. We truly did magic on the vision end, in that we optimised the code in every imaginable way (for example, we raytrace with one function call to OpenCV’s drawRectangle, rather than multiple calls to drawLine in a for loop). 3D vision would have required much better hardware.
        <br>
        <br>

        Exploration algorithm + control:
        <br>
        The main reason for choosing the flood fill algorithm was that it searches every single open space, while avoiding all obstacles. It also lets us constantly update the occupancy grid as we fly. The UAV control it uses (moving forward/backward/left/right one cell at a time) also integrated well with the DJI SDK’s control API. In addition, and very importantly, the flood fill algorithm is very robust, reliable, and easily visualized. We could have tried using a machine learning model, but those aren’t as robust, and we can’t know for sure why the UAV chose its next move. With the flood fill, we know how it chooses the next move, and can predict what it’ll do next. With an expensive physical UAV, this is highly valuable as we can’t risk any unpredictable behavior that can cause a crash.
        <br>
        <br>

        <u>
        (d) How do these design choices impact how well the project meets design criteria that would be encountered in a real engineering application, such as robustness, durability, and efficiency?
        </u>
        <br>
        Overall we did quite well.
        <br>
        - The vision system is robust, despite being deployed in somewhat unreliable hardware. The quadrotor loses critical sense of sight only when an object is within 0.7 m in its FOV, which is acceptable. The median filter is able to remove most of the noise in the disparity image, giving an accurate and usable reading.
        <br>
        - The map is updated at 9~10 Hz, which effectively means that our vision system is extremely, extremely efficient.
        <br>
        - When it comes to mapping methods, occupancy grids are much more memory-heavy than meshes, but offer faster operation runtimes. That said, in 2D, all memory impacts are minimal, signifying that the occupancy grid is the superior choice, offering the best of both worlds in terms of memory and runtime efficiency.
        <br>
        - Explore efficiency can be increased by gaining information about all directions at every cell by adding cameras or rotating the UAV around
        Usage of M210 UAV and SDK ensures durability and scalability to other UAVs.
        <br>
        - VR architecture supports scalability to multiple UAVs.
        <br>
        - VR visualizations highly effective/ easy to understand whats going on
        <br>
        <br>

      </p>
      <br>

      <h1 class="display-4"> Implementation </h1>
      <p class="lead">
        <br>

        <u>
        (a) Describe any hardware you used or built. Illustrate with pictures and diagrams.
        </u>

        <br>
        Drone:
        <br>
        Drone: DJI Matrice 210
        <br>
        Onboard sensors: RTK GPS (50 Hz), Gyroscope (100 Hz), Stereo Cameras (10 Hz, 240p, 60×54° FOV)
        <br>
        Onboard computer: Manifold 2-C
        <br>
        Software: DJI Onboard SDK
        <br>
        On the ground: RTK base station (communicates with on-board RTK GPS for centimeter-accuracy location data)
        <br>
        <br>

        VR:
        <br>
        We used the Facebook Oculus Rift as the Virtual Reality Headset which comes equipped with two handheld controllers. The virtual reality application requires a VR ready laptop with the ability to support the Oculus headset, and we used the Alienware M15 R3 laptop to deploy the virtual reality application.
        <br>
        <br>

        <u>
        (b) What parts did you use to build your solution?
        </u>

        <br>
        Commercial off the shelf parts listed above
        <br>
        <br>

        <u>
        (c) Describe any software you wrote in detail. Illustrate with diagrams, flow charts, and/or other appropriate visuals. This includes launch files, URDFs, etc.
        </u>

        <br>
        Web App
        <br>
        The web application was built using Flask, HTML, CSS and Bootstrap. It is a 2 page website with the first page comprising the project report created using HTML/CSS. The second page has an input form using Bootstrap components and uses an HTTP POST request to send this form to the virtual reality application. This is deployed on Heroku which abstracts away all hosting code.
        <br>
        <br>

        VR:
        <br>
        The virtual reality application was built using Unity, C#, the ROSBridgeWebsocket library and numerous Unity assets. We used the .NET unity library to create a REST API endpoint for the server to send the user information to with C# code to parse the response and visualize a pointer for the VR operator to be informed of the user’s general location. Unity assets for 3D buttons along with C# scripts are used to create the user interface to enable operators to create and define a search region in the virtual reality interface. The UAV class to create the virtual UAV gameobject is written to spawn a websocket connection to the UAV’s onboard computer using the ROSBridgeWebsocket library. This class subscribes to the UAV’s position, localizes it and constantly updates the VR user interface to inform the operator of a UAV’s real-world position. This class also contains all the functionality to call ROS services hosted on the UAV’s onboard computer and is used to trigger functionality as per the VR operators needs. We further created the abstract custom service calls and message types classes that can scale as required.
        <br>
        <br>


        ROS “isaacs_autonomy” Node essentially does 3 things: All written in Python
        <br>
        1. Sensor Streams: Explained Above
        <br>
        2. Image Processing: Explained Above
        <br>
        3. Occupancy grid: Explained Above
        <br>
        4. Search Algorithm:
        <br>
        The search algorithm takes in a radius in meters as a parameter. It then attempts to traverse every cell in that radius, unless there is an obstacle. It uses the occupancy grid to understand the environment (is a space occupied/unoccupied/unknown), and has its own 2d array of visited/unvisited cells.
        <br>
        It is a recursive algorithm that checks the appropriate neighbor’s cell and if it's unoccupied+unvisited:
        <br>
        - Moves up, and recurses.
        <br>
        - Moves down, and recurses
        <br>
        - Moves right, and recurses
        <br>
        - Moves left, and recurses
        <br>
        It is a depth first search as it visits a cell’s neighbors’ neighbors before visiting all of its own neighbors. It also updates the occupancy grid as the UAV moves, to ensure we have up-to-date info about obstacles.
        <br>
        <br>

        <u>
        (d) How does your complete system work? Describe each step.
        </u>
        <br>
        1. A customer places an order with their world coordinates.
        <br>
        2. An operator in VR creates a waypoint mission for the quadrotor to the user area.
        <br>
        3. The operator in VR creates a search area and uploads the mission to the quadrotor
        <br>
        4. The quadrotor flies to the target area, following the waypoint mission.
        <br>
        5. The quadrotor autonomously searches the target area by creating an occupancy grid of obstacles using onboard cameras and performing a depth first search.
        <br>
        6. [Ran out of time] The quadrotor detects a QR code indicating the precise location of the delivery and lands.
        <br>

      </p>
      <br>

      <h1 class="display-4"> Project Results </h1>
      <p class="lead">
        <br>

        <u>
        (a) How well did your project work? What tasks did it perform?
        </u>

        <br>
        Overall, we are very happy with the progress we made over the course of the semester. All our key components (VR, Vision, Search, Hardware) are successfully integrated and worked. We wish to conduct a real-world flight test when permitted by COVID-19 regulations but based on the success in the realistic DJI sim we are confident that our application will succeed.
        <br>
        <br>

        <u>
        (b) Illustrate with pictures and at least one video
        </u>
        <br>
        </p>
        <div class="embed-responsive embed-responsive-16by9">
          <iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTX-ST1ma7Gus38mT3FtR6tQWuuuqaTFFEDIdOWAPMphmtRBFyBLJKavII0yZ4iiUTYGmzwPh8y1LBT/embed?start=false&loop=false&delayms=10000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
        </div>
        <br>

<!--
        <p class="lead">
          <br>
          A video demo of the full project: (TODO: Update)
        </p>

        <div class="embed-responsive embed-responsive-16by9">
          <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/gZLGlP98EsA" allowfullscreen></iframe>
        </div>

        <br>
-->

        <br>
        <br>

      <h1 class="display-4"> Conclusion </h1>
      <p class="lead">
        <br>

        <u>
        (a) Discuss your results. How well did your finished solution meet your design criteria?
        </u>

        <br>
        Overall, we are very happy with the progress we made over the course of the semester. All our key components (VR, Vision, Search, Hardware) are successfully integrated and worked. We wish to conduct a real-world flight test when permitted by COVID-19 regulations but based on the success in the realistic DJI sim we are confident that our application will succeed.
        <br>
        We understand that flying in sim can sound disappointing, and obviously it's different from flying in the real world. But at the same time, we want to emphasize a few things:
        <br>
        The DJI Assistant 2 For Matrice simulator is not just any simulator, it is an industry grade simulator created by DJI themselves specifically for their UAVs. The Simulator only works when plugged into the physical UAV as all physics, flight controller, measurement and other calculations take place on the UAV’s flight controller itself. The flight controller is publishing and supporting all the same ROS functionality for control as a real flight, with only the rotors and gps data being simulated. In previous projects, after achieving successful flights in the DJI simulator, they have always transferred into successful real-world tests and therefore we are confident that we can achieve a full demo flight when we are able to conduct one.
        <br>
        <br>

        <u>
        (b) Did you encounter any particular difficulties?
        </u>

        <br>
        1. Calibrating real world camera is annoying
        <br>
        2. Drone cameras were not the best
        <br>
        3. Creating local networks for UAV flights is painful
        <br>
        4. Setting up and testing with the DJI sim is also painful
        <br>
        5. Testing the DJI SDK was painful
        <br>
        6. Limited access to equipment/only 2 days in RFS
        <br>
        7. When we did have access, had to keep recharging batteries (UAV batteries aren't great)
        <br>
        8. Limited daylight to fly UAVs
        <br>
        9. GPS hardware localization was spotty
        <br>
        10. Flight checklist/safety precautions
        <br>
        - Can’t just go out and fly autonomously
        <br>
        - Cant make small changes and keep testing
        <br>
        - With ML models, you crash a million times before it works once
        <br>
        - With physical, expensive UAVs, has to work every single time
        <br>
        - Need robust calibration
        <br>
        - x/y flight test
        <br>
        - Set waypoints directly above 4 corners of the building, and make sure it lines up
        <br>
        - Need spotters from different angles because depth perception is weird
        <br>
        - Altitude test
        <br>
        - Set waypoints at window height to make sure altitude matches
        <br>
        - Now that we trust x/y/z conversion between real world GPS and our Unity VR interface, we can actually run flight tests
        <br>
        <br>

        <u>
        (c) Does your solution have any flaws or hacks? What improvements would you make if you had additional time?
        </u>
        <br>
        Every time we fly needs re-calibration of GPS with our VR interface. Even though our interface has fixed coordinates, and the GPS gives us absolute real-world coordinates, for some reason they wouldn't line up. As an example, if we place the UAV by the corner of a building, and convert to our unity coordinates, it correctly shows up in our unity visualization. But if we turn everything off, and then on again, the UAV shows up 15 feet off in our visualization, so we have to recalibrate everything.
        <br>
        <br>

      </p>
      <br>


      <h1 class="display-4"> The Team </h1>
      <p class="lead">
        <br>
        The following UC Berkley students developed this project for the EECS 106A Class.
      </p>

      <br>

      <div class="card-deck">
        <div class="card">
          <img class="card-img-top" src="{{ url_for('static', filename='peru.jpg') }}" alt="Card image cap">
          <div class="card-body">
            <h5 class="card-title">Peru Dayani</h5>
            <p class="card-text">
            Project Manager and VR/Web developer
            <br>
            - VR Developer
            <br>
            - Web Developer
            <br>
            - Project Manager
            <br>
            - Way too much Debugging
            </p>
            <p class="card-text">
            EECS @ UC Berkeley
            </p>
            <a href="https://www.linkedin.com/in/peru-dayani-09367314a/" class="btn btn-primary">LinkedIn</a>
          </div>
        </div>
        <div class="card">
          <div style="width:349px ;height:350px ;overflow:hidden">
            <img class="card-img-top" src="{{ url_for('static', filename='varun.jpeg') }}" alt="Card image cap">
          </div>
          <div class="card-body">
            <h5 class="card-title">Varun Saran</h5>
            <p class="card-text">
            Search Algorithm and Hardware
            <br>
            - Search Developer
            <br>
            - Hardware Manager
            </p>
            <p class="card-text">
            EECS @ UC Berkeley
            </p>
            <a href="https://www.linkedin.com/in/varunsaran/" class="btn btn-primary">LinkedIn</a>
          </div>
        </div>
        <div class="card">
          <div style="width:349px ;height:350px ;overflow:hidden">
            <img class="card-img-top" src="{{ url_for('static', filename='Apollo.png') }}" alt="Card image cap">
          </div>
          <div class="card-body">
            <h5 class="card-title">Apollo θ</h5>
            <p class="card-text">
            <br>
            - Hardware Setup
            <br>
            - ROS Integration
            <br>
            - Sensor Calibration
            <br>
            - Sensor Streams
            <br>
            - Image Processing
            <br>
            - Occupancy Grid
            <br>
            <p class="card-text">
            CS @ UC Berkeley
            </p>
            <a href="https://apollovision.github.io/" class="btn btn-primary">Website</a>
          </div>
        </div>
      </div>

      <br>
      <br>
      <br>

      <h1 class="display-4"> Additional Materials </h1>
      <p class="lead">
        <br>
        Autonomy Package: <a href="https://github.com/apollovision/isaacs_autonomy"> Link </a>
        <br>
        VR Interface: <a href="https://github.com/immersive-command-system/ImmersiveDroneInterface_2/tree/106a-proj"> Link </a>
        <br>
        WebApp: <a href="https://github.com/apollovision/robotics_final_project/tree/master/WebApp">Link</a>
        <br>
        M210:
        <br>
        Product: <a href="https://www.dji.com/matrice-200-series">Link</a>
        <br>
        Specs: <a href="https://www.dji.com/matrice-200-series/info#specs">Link</a>
        <br>
        RTK Station:
        <br>
        Product: <a href="https://www.dji.com/d-rtk-2">Link</a>
        <br>
        Manifold 2C:
        <br>
        Product: <a href="https://www.dji.com/manifold-2">Link</a>
        <br>
        Specs: <a href="https://www.dji.com/manifold-2/specs">Link</a>
        <br>
        DJI Simulator:
        <br>
        Product: <a href="https://www.dji.com/downloads/softwares/assistant-dji-2-for-matrice">Link</a>
      </p>
      <br>

    </div>

    {% endblock %}
  </body>
</html>
